---
title: PhD journal entry
date: "2025-07-11"
---

This week I spent a lot of time implementing the final tools needed to evaluate properly my models
in the torch repository. As a reminder, I departed from the original BQ-NCO repository because their
code is a little bit messy and hard to modify. Specifically, I wanted an efficient version of my
modified self-attention (ALiBi-like). I went into my beloved JAX just to find out it wasn't
compiling an efficient operation so I went back to PyTorch's flex attention and made it somehow
working pretty well. Flex attention is not easy to use and we can feel that it's still in
production, but it's a very good piece of software and work very well!

I think that I now have totally finished most of the things required to train and evaluate my
models. I trained an ALiBi model with ssmax and random ids on 1G samples (equivalent to 1000
epochs). This ALiBi model is slightly different than the previous ones because its slopes are not
learnable, which I think makes it easier to generalize and makes the whole training simpler (no need
for an extra regularizing loss). Similarly, the ssmax has no learnable scalar value. And finally
instead of training over 2000 epochs like previously, I train only on the equivalent of 1000 epochs.
This new implementation allows me to randomly sample the step of the partial solution that the model
is being trained for each element of the batch separately. It enforces padding of size 128 (flex
attention) but I believe each batch is less biased and more informative than in the original BQ-NCO
repository where each sample is at the same partial solution step.

Now it turns out this new ALiBi model is not as good as my previous one on small TSPs, but it's
similar or even slightly better than my previous best ALiBi model on TSP-1000. I also have RoPE
ongoing and a replicate of their approach with coordinates. I wonder how they will perform. I maybe
will have a hard time with my supervisors for my next meeting, as they're expecting me to have
stopped experiments. I need those results before the next meeting, which I think I can.

It's maybe the first time that I properly compare ALiBi, RoPE and the coordinates approach. I'm
surprised how similar the training curves are. I really thought ALiBi and RoPE would beat the coords
by a large margin but that does not seem to be the case. Training runs are not finished yet so we'll
see how it goes. I guess the only way to make real progress is to have more deeper changes.

Also I read a paper about Energy-based Transformers, that linked EBMs, reasoning and LLMs! All the
good stuff. They took the same approach as previous papers that used EBMs to solve reasoning tasks
but for common tasks such as next word prediction and image generation. They also showed some useful
tricks to stabilize the EBM training. They didn't use any implicit diff. I don't think any EBM work
did for now!
