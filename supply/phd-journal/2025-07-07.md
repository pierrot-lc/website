---
description: PhD journal entry
tags: "2025-07-07"
---

This week I finished the Deep Learning Summer School, and oh boy how I hated that last day. It was a
summary of the overall feeling I got from the school: the organizers are AI doomers/haters.

We finished with nothing more than [Stuart Russel](https://en.wikipedia.org/wiki/Stuart_J._Russell).
I was so happy to see the man behind the textbook. Turns out he's a doomer!! He basically finished
his talk by saying that humanity will probably not survive AI.

That's amazing to me. Whether it's him or the organizers, they both work in Deep Learning and
contribute to this field everyday. Yet, when they talk about their field, there's only negative
words that come out of their mouths. They love to criticize, yet I believe they propose nothing.

How can this be possible? Why such people criticize their work so hard? Is it because they used to
work in AI before the Deep Learning wave, and never admitted their previous work was outdated? Or
maybe it's because they really are scared about what their own work could do, and they would like to
have their mind cleared, something like "I tried to tell them"? Or it's because it's trending, it's
the easy way to get people's attention. "Look! Even this very famous AI researcher is saying that we
should be scared of AI!" And even sometime I wonder if they're not a little bit happy to say that
what they did is so powerful it could destroy humanity. Like not for real but they like that people
may belive it.

Something else that really astonished me, is the low scientificity in its talk. How can it be
possible for such a huge scientist to give a talk about AI doom without giving true numbers,
estimations and studies! It was only made up numbers, small ideas such as "the AI could hack and cut
the whole electricity grid of a country", or hear-say "I talked to one CEO of a huge LLM company,
\[...\] we should be happy if the only thing we get is Nagasaki".

A lot of things was contradictory. He said the classical "AI is idiot and ever will be", and 5
minutes later, "AI will surpass any human on earth and will self-evolve". How??? Another example:
"AI should be a rational entity to be super-human", and then "The human is not rational, so AI
shouldn't be rational as well!". It went in every directions, we learnt nothing except that we are
doom, but that somehow AI is unable to generalize to new situations.

Never meet your heroes I guess. I will attend to another summer school in Grenoble in early
September, and I will also attend to a talk to Yann LeCun, my personal guide in AI. He is the person
with whom I agree the most, and I hope it will be much better than this last talk I just attended.

Props to HuggingFace for their talk, it was the only technical talk that was pleasant to follow. The
other technical talks were too fast, it wasn't very deeply technical but more a very quick
presentation about every published paper by a senior researched for its last 3 years.

Anyway, other than that, I still struggle to train my models in time. I feel like I loose so much
time. I'm not patient. I don't know how to set the hyperparameters. Yet my supervisors told me that
I should start writing a paper froze my results. But I don't want.. I'd like to train my model with
my training code. I'd like to beat the BQ-NCO model cleanly...
