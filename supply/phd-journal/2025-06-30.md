---
title: PhD journal entry
date: 2025-06-30
---

I'm currently attending the [Deep Learning
School](https://univ-cotedazur.eu/efelia-cote-dazur/summer-schools/deep-learning-school/deep-learning-school-2025).
It's not as great as I expected and I end up working a lot more than I thought on my research
project rather than listening to the talks and the tutorials. It's too much LLM oriented to my
taste.

Also the person doing the tutorials is so spicy when talking about deep learning. It feels like he
don't even like the field, always talks about how LLMs are eating an enormous amount of data, how
non-ethical its training is, and how closed-source the field is (it is not). From what I heard from
people that knows him, he really love the tech so I am unsure why he keeps reminding us that this
field is so dangerous and unethical. He could have talk about something else than LLMs, the field of
deep learning is vast.

Anyway, I've had the time to work on my research project and my goal is to have a training that is
*fast* and *efficient*, as much as possible. I obviously went with JAX, mainly because I need to be
able to use custom bias in the self-attention operation and JAX propose it natively. It was great,
running at 5000 samples/seconds, until I tried my code on the lab's machine. It was running only at
1500 samples/seconds!! After some investigations, I understood that it was a memory issue. The GPU's
compute was always waiting for the GPU's memory to be done with setting up the data. I guess my RTX
5090 has a memory bandwidth that is more than enough and so it was capable of running at 90+% of its
compute capability.

The solution seems to use flash-attention. But with my special need for custom bias there are no
implementation available for JAX. So I rewrote everything for PyTorch, using their new
*flex-attention* operation that allows for custom self-attention implementation. Oh boy it was hard.
Using flex-attention looks easy when looking at their [introducing
blogpost](https://pytorch.org/blog/flexattention/), but in reality you end up with a bunch of
compilation errors and NaNs gradients. Here's all the issues I encountered:

- You cannot compile and run on GPU a score_mod function that closes on more than 1 tensor.
- The token's dimensions must be a power of 2.
- You cannot compute the gradient of your attention if the sequence length is not a multiple of
  128.

Come on.

That's where JAX shines. Once you understood the requirement that your function need to be pure, the
rest is done for you and is done powerfully (except for flash-attention I guess, or at least on old
GPUs?). At least I am now playing a little bit with `torch.vmap` and `torch.compile`, two of the
most powerfull tools from JAX, and when it works it's a pleasure.

Surprisingly, even on a H100 I get only 4100 samples/seconds :(.

I now have the following training code:

- Random sequences of TSP (with different sequences length for each sample of the batch).
- ALiBi, RoPE, coords, random ids.
- Resumable training!
- A compiled self-attention, which I hope will help me when evaluating the model on TSP-10k.

I attented the presentations from hugging face and it was amazing. I learnt about the multiple ways
you *shard* your compute on multiple GPUs, and the latest and greatest tricks to train LLMs. I had a
small discussion with Elie Bakouch after its presentation and even him is doing experiments that
last no more than 8 hours (he trains LLMs!). Mines take 3 days!! I should absolutely find a way to
do mine at that pace. Maybe by using my brand new PyTorch training code, and training on TSP-20?

My RoPE models are highly sensible to the scaling factor. This is expected as it works highly
differently from the ALiBi. Still, with a small scaling factor such as 1.1, the model is doing
better. What's even more strange is that the original paper is also doing better with this trick!
It's starting to be very difficult to beat the original model..

I still need first to make sure my implementation is working similarly than the previous one.
